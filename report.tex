\documentclass[english,10pt,twocolumn]{article}

\usepackage{times}
\usepackage{fullpage}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{pgfplots}


\begin{document}

\title{A Unikernel Key-Value Store}
\author{Nate Brennand, Gudbergur Erlendsson, Mishail Tupas}
\date{May 6, 2015}
\maketitle
\thispagestyle{empty}


\begin{abstract}
Traditional web infrastructure models consist of a cluster of heavyweight full stack virtual machines usually running a variant of Linux.
This provides a simple consistent platform for developers to write their applications against but running a full stack machine on top of a hypervisor has a lot of resource overhead.
Key-value datastores are a common component of web infrastructure.
We've designed a Mirage OS\cite{mirage} unikernel implementation of the popular Redis\cite{redis} in-memory key-value store that allows fast creation and deployment of the datastore as well as ease scaling up the allocated resources.
Applications developed with MirageOS can be deployed directly on the Xen hypervisor.
We describe the advantages and disadvantages that come with such a system.
We highlight the performance characteristics of this new approach in comparison to Redis.

Preliminary results showed lower-than-expected performance.
We expect with more profiling and testing we can identify the hotspots in our codebase and garner better performance from the Keyvee implementation.
\end{abstract}


\section{Introduction}
In-memory key-value datastores are used in a wide variety of applications for ease of use and the high throughput.
While these applications are known for their great performance, research has shown that networked applications striving for low latency, such as Redis, are actually subjected to a high amount of overhead from utilizing Linux's networking stack.\cite{arrakis}
This issue is amplified by the lack of stability in allocated resources when running on Linux itself which is prone to performance dips when the resource scheduler is not coordinated.

We believe that the approach of moving the key-value datastore to a unikernel implementation will yield greater and more reliable performance.
A unikernel is a specialized application capable of running directly on top of bare-metal or a hypervisor without a host operating system.
The unikernel approach lacks many of the performance issues plaguing Redis that cannot be resolved at the application level.
We build on top of the MirageOS\cite{mirage}, an OCaml based library operating system.
MirageOS provides correctness due to the strongly-typed nature of OCaml, this yields very reliable code.

We measure the performance characteristics of Redis in multiple situations: networked and on-host, to yield a direct comparison to our implementation, Keyvee.
Keyvee speaks the Redis Serialization Protocol\cite{redis-protocol}, RESP, which makes Keyvee a drop in replacement for Redis in any deployment utilizing the Xen Hypervisor.
We hope this compatibility will allow Keyvee to help establish unikernels as a viable piece of web application infrastructure.


\section{Background}



\subsection{Xen Hypervisor}

The Xen hypervisor is a tool to allow a host to simultaneously host many virtual machines on the same physical host.
Xen allows performant configurations of 100's of virtual machines running in parallel on virtualized resources.
Standard operating systems can be run on Xen with minimal changes to the codebase.
For applications running on a host, such as Linux, Xen has no impact on the implementation because all of the integrations are completed at the OS layer.


\subsection{Redis}

Redis is a data structure server and is the most popular key-value store in industry\cite{dbengines}.
Its working set and all of its data is in-memory at all times, and there is optional durability using append-only log files.
It has various applications in server infrastructure, including caching, inter-server communication, queues etc.
Redis maps keys to numerous types of values, including strings (supported by Keyvee) as well as lists of strings, sets of strings, hash tables etc.
Redis has support for master-slave replication, and allows slaves to be masters to other slaves, enabling Redis to implement single-rooted replication tree.
When durability is not needed, Redis is very performant, although according to tests it has been shown to spend upward of 84\% of it's processing in kernel space, meaning there is some space to improve it's performance \cite{latency}.


\subsection{MirageOS}

MirageOS is an OCaml-based library operating system from the Xen project that compiles to a Xen hypervisor image.
This approach yields several performance and implementation benefits.
Mirage applications are entirely type-safe due to the strong typing of OCaml, they also have a very compact size due to the lack of structural libraries that must be included.
Also, with the lack of an operating system interface layer, Mirage applications achieve top-notch performance as there is no additional layer in the IO pipeline or scheduler to delay the application's execution.

\begin{figure}[ht]
  \centering
  \caption{MirageOS architecture}
  \includegraphics[width=0.5\textwidth]{images/design}
\end{figure}

Thanks to its simple runtime, OCaml, an expressive functional systems programming language, was a good fit for the Mirage project, and was used to re\-implement the runtime in an image that would run on Xen.
The strictness of OCaml's type system ensures that programs are respect with regard to variable data types prior to compilation, because of this strictness the executable is able to shed all type checking which yields fast native code with performance comparable to C.

As highlighted in Figure 1, unikernels have a greatly reduced overhead when accessing resources relative to an application running on Linux.
This approach brings some great benefits like zero-copy IO and no scheduler-induced delay in response.
These performance improvements are important in high throughput and low latency applications like key-value datastores.
However, this approach does have downfalls such as Mirage's limit to a single virtual machine per core; this limitation means that certain applications that require access to multiple physical threads will not fit the Mirage programming model.
For applications that do not need multiple physical threads but need asynchronous IO, Mirage has a lightweight cooperative threading, LWT, library that provides simple constructs for asynchronous code.

Simple experiments with Mirage have proven that competitive performance to native C applications is not difficult to achieve.
For example, a Mirage DNS server implementation outperformed the standard Bind9 and NSD servers by 14\% in raw throughput in testing.
This indicates that for the right workload, Mirage is an attractive choice that should be evaluated.

In addition to the performance benefits, unikernels are inherently more secure than a standard operating system.
The smaller the attack surface area, the more difficult it is to compromise an application.
When a Mirage image is compiled, only the imported libraries are included which means that only the necessary ports and protocols are exposed.
This aspect, combined with the type-safety OCaml provides eliminates entire classes of security vulnerabilities in Mirage applications.


\subsection{Related Work}

HaLVM\cite{halvm} and LING\cite{ling} are both alternatives to MirageOS, using Haskell and Erlang respectively as the basis of their unikernel efforts.
Each of the unikernel implementations: HaLVM, LING and Mirage, share the trait that you are limited to one functional language exclusively and compile into an image to run on top of the Xen hypervisor.
MirageOS and LING focuses on performance and predictability while HalVM focuses more on elegant compositional semantics using Haskell, according to the authors\cite{tripreport}.
Low level libraries are being implemented for the platforms in their native code, so while the work is ongoing the level of support varies.
MirageOS for examples uses OCaml-DNS, Mirage-TCPIP and other libraries. It also has a relatively advanced TLS library (OCaml-TLS) and better support for encrypted traffic then the other two library operating systems mentioned earlier.

OSv\cite{osv} is another variant to the special-purpose operating system using the library OS paradigm, mostly based on code from BSD.
In OSv everything runs in the kernel space and it's built to run as guest on top of a virtual machine.
However, it supports Linux ABI, and contains necessary functionality to run Java and POSIX applications which allows OSv to run some existing codebases and get some of the performance benefit Mirage enjoys, but not increased security.
To get zero-copy I/O specific non-POSIX OSv API's need to be used.
Mirage however does not focus on support for legacy applications but rather focuses on a toolkit to assemble new unikernel applications without specific domain knowledge in kernel programming.

Arrakis\cite{arrakis}, SPIN\cite{spin} and Exokernel\cite{exokernel} all reduce shared kernel components and allow each application to have customized operating system management, achieving some of the increased performance benefit of MirageOS without tying you to one language runtime.
Arrakis in particular benefits from increased support for virtualization in new hardware, which allows it to effectively run all applications in kernel space and achieve separation using hardware instructions, similar to how MirageOS achieves separation through Xen.

\section{Design and Implementation}
For Keyvee we have the following goals:
\begin{itemize}
  \item Implement subset of Redis functionality in OCaml on top of the MirageOS toolchain
  \item Make Keyvee's as compatible to Redis as possible.
  \item Implement RESP, Redis's protocol specification, to allow interoperability with existing tools and libraries.
\end{itemize}

Here in this section we will talk about our implementation of Keyvee and how we try to accomplish these goals.

\subsection{Overview}

\begin{figure}[ht]
  \centering
  \caption{Supported commands}
  \includegraphics[width=0.5\textwidth]{images/commands}
\end{figure}

Keyvee is implemented on top of the MirageOS toolchain using OCaml.
Currently, we support the GET, SET and DEL commands.
All data is stored in-memory, similar to Redis.
We have an internal module implementation built on top of OCaml's Hashtbl module, which stores our keys and associated data in an efficient lookup table.
We use a hash table with a seeded hash function, using a seed randomly chosen at hash table creation time.
This means the hash function is randomly selected among 230 different hash functions with different collision patterns which helps stop denial-of-service attacks where attackers can send input crafted to create collisions, slowing our application to a halt.

We use MirageOS's low level TCP/IP network stack implemented in pure OCaml.
Because memory layout in Mirage is used to distinguish I/O pages, the OCaml garbage collector needs to scan less to be as effective as in a full stack OS, which let's the network stack perform predictably.
Predictable performance, as well as support for zero-copy I/O makes Mirage's network stack more performant than traditional full stack OS.
The network stack then hands off the raw data to our parse functions, which parse the data using string operations.
The parse functions fully support well formed queries for our supported functions, but lack some verification steps and will not respond to badly formed queries.
After the parse phase, the data is handed off to the Hashtbl module which responds with data or writes data.
The response is then written to the client in RESP compatible format.

Mirage uses the Lwt, cooperative threading library, extensively and every request is wrapped in a lightweight cooperative thread. Leveraging Mirage and OCaml guarantees type safety in both our application as well as in the implementations of frameworks underlying the application such as the TCP/IP stack, enhancing the security of our implementation.


\subsection{Development process}

We use distributed version control system Git and Github to track changes to our codebase.
Travis Continuous Integration is then used to build our codebase when changes are made and check for any errors.
It is possible to build a Unix binary using the Mirage toolchain, which we use extensively when developing our code before it is deployed and tested on the Xen server.

The MirageOS documentation leaves a lot to be desired and many times we needed to go looking into MirageOS's implementation it self.
In addition, some OCaml modules do not work with the toolchain and there is scarcity of open source code built with it.
Especially as it is the early days there is not a lot of community support surrounding Mirage.


\section{Evaluation}
\subsection{Testing Environment}
We performed all of our evaluations on a dedicated 8-core HP ProLiant rack server with 16 GB RAM available.
Our Xen hypervisor installation is coordinated by a Ubuntu Server 14.04 host that serves as the management domain to manage the other hosts running on the Hypervisor.
The Mirage installation was modified to use Redis's communication protocols so that we could use the same benchmark for all test setups. The benchmark was run using 50 parallel clients sending a 3-byte payload.
The Redis setups were given ten thousands requests as part of their benchmark, while the Mirage setups on Xen were given five hundred, one thousand, or ten thousand requests.

\subsection{Running Mirage}


When a MirageOS unikernel is configured in Xen mode, both the unikernel image and the configuration file are generated.
With these, a unikernel can be easily launched using the xl Xen management tool.
Due to the small size of the image, this is a quick process where the Mirage image configures itself when started by xl.
We used a Redis 2.8 installation on Xen as the comparison application to Keyvee.
The test clients were hosted by the supervising host on Xen.

Using the Redis benchmark suite to examine Keyvee provided guarantees that the Redis communication protocols were properly fulfilled and provided a common means of measuring throughput.
Four commands were implemented and tested: SET, which assigned a single key its value; MSET, which assigned values to multiple keys; GET, which retrieves the value of a given key; and PING, which tests the server's connectivity.

Three aspects of scalability were tested: raw request volume, request parallelization, and request pipelining.
To test scalability with raw volume, a benchmark setup would have either a total of one million or five million of each command.
For parallelization, the remote host would set up a group of 10, 100, 500, one thousand clients.
Regarding pipelining, requests were pipelined with either 1, 10, or 50 requests sent per packet.
Each benchmark setup, in conjunction with a key space size of 1 and a key size of 20 bytes, used a combination of these three aspects, resulting in a total of 16 different tests.

\subsection{Throughput Performance}

Increasing the number of total requests had no significant effect on the relative throughput of Keyvee when compared to Redis.
Pipelining proved to scale better with Keyvee, but it's benefit was overshadowed by the profound effect scaling the number of concurrent clients had on throughput
Across all the implemented commands, Keyvee performed just as well, if not better, than Redis when very few clients were connecting at once.
However, Keyvee's throughput decreased logarithmically as the number of simultaneous clients increased, and would have consistently lower throughput than Reids after a very low threshold, with some variance between commands.
Upon further examination of the code and testing, it was found that the overhead from the light-weight threads used by Mirage to handle simultaneous connections resulted in the severely reduced throughput.

\subsection{SET Command}

\begin{figure}[!htb]
\begin{tikzpicture}
\begin{axis}[
xlabel = {Parallel Clients},
ylabel = {Throughput (10\textsuperscript{5} Requests/sec)},
legend columns = 3,
transpose legend,
legend style={at={(0.5,-0.2)},anchor=north}]
\addplot[
color=blue,
mark=diamond,
]
table{./results/redis/SET__pipeline_1_.results};
\addlegendentry{Redis, no pipeline};
\addplot[
color=blue,
mark=square,
]
table{./results/redis/SET__pipeline_10_.results};
\addlegendentry{Redis, pipeline of 10};
\addplot[
color=blue,
mark=triangle,
]
table{./results/redis/SET__pipeline_50_.results};
\addlegendentry{Redis, pipeline of 50};
\addplot[
color=red,
mark=diamond,
]
table{./results/keyvee/SET__pipeline_1_.results};
\addlegendentry{Keyvee, no pipeline};
\addplot[
color=red,
mark=square,
]
table{./results/keyvee/SET__pipeline_10_.results};
\addlegendentry{Keyvee, pipeline of 10};
\addplot[
color=red,
mark=triangle,
]
table{./results/keyvee/SET__pipeline_50_.results};
\addlegendentry{Keyvee, pipeline of 50};
\end{axis}
\end{tikzpicture}
\caption{SET Throughput}
\end{figure}
\begin{figure}[!htb]
\begin{tikzpicture}
\begin{axis}[
xmode = log,
xlabel = {Maximum completion time},
ylabel = {Percent of all requests},
legend columns = 4,
transpose legend,
legend style={at={(0.5,-0.2)}, anchor=north}]
	\addplot[
	color=blue,
	mark=diamond,
	]
	table{./results/redis/SET_pipeline_1_w_10.cdf.results};
	\addlegendentry{Redis, 10 clients};
	\addplot[
	color=blue,
	mark=square,
	]
	table{./results/redis/SET_pipeline_1_w_100.cdf.results};
	\addlegendentry{Redis, 100 clients};
	\addplot[
	color=blue,
	mark=triangle,
	]
	table{./results/redis/SET_pipeline_1_w_500.cdf.results};
	\addlegendentry{Redis, 500 clients};
	\addplot[
	color=blue,
	mark=star,
	]
	table{./results/redis/SET_pipeline_1_w_1000.cdf.results};
	\addlegendentry{Redis, 1000 clients};
	\addplot[
	color=red,
	mark=diamond,
	]
	table{./results/keyvee/SET_pipeline_1_w_10.cdf.results};
	\addlegendentry{Keyvee, 10 clients};
	\addplot[
	color=red,
	mark=square,
	]
	table{./results/keyvee/SET_pipeline_1_w_100.cdf.results};
	\addlegendentry{Keyvee, 100 clients};
	\addplot[
	color=red,
	mark=triangle,
	]
	table{./results/keyvee/SET_pipeline_1_w_500.cdf.results};
	\addlegendentry{Keyvee, 500 clients};
	\addplot[
	color=red,
	mark=star,
	]
	table{./results/keyvee/SET_pipeline_1_w_1000.cdf.results};
	\addlegendentry{Keyvee, 1000 clients};
	\end{axis}
	\end{tikzpicture}
	\caption{SET Latency w/o pipelining}
\end{figure}
Without pipelining, SET throughput was roughly equal when there were few concurrent clients, but as pipelining increased, Keyvee's throughput improved significantly. 
However, gains from pipelining would be eventually offset by the scaling issues of handling concurrent clients.
Interestingly, at a pipeline length of 10 the biggest difference in scaling was found, since the benefits for a 50-long pipeline managed to maintain Keyvee's throughput somewhat.
When comparing latency of the commands, Keyvee's latency only slightly behind Redis, but as the number of concurrent clients grew, the disparity in latency grew wider.

\subsection{MSET Command}

\begin{figure}[!htb]
\begin{tikzpicture}
\begin{axis}[
xlabel = {Parallel Clients},
ylabel = {Throughput (10\textsuperscript{5} Requests/sec)},
legend columns = 3,
transpose legend,
legend style={at={(0.5,-0.2)},anchor=north}]
\addplot[
color=blue,
mark=diamond,
]
table{./results/redis/MSET__10_keys___pipeline_1_.results};
\addlegendentry{Redis, no pipeline};
\addplot[
color=blue,
mark=square,
]
table{./results/redis/MSET__10_keys___pipeline_10_.results};
\addlegendentry{Redis, pipeline of 10};
\addplot[
color=blue,
mark=triangle,
]
table{./results/redis/MSET__10_keys___pipeline_50_.results};
\addlegendentry{Redis, pipeline of 50};
\addplot[
color=red,
mark=diamond,
]
table{./results/keyvee/MSET__10_keys___pipeline_1_.results};
\addlegendentry{Keyvee, no pipeline};
\addplot[
color=red,
mark=square,
]
table{./results/keyvee/MSET__10_keys___pipeline_10_.results};
\addlegendentry{Keyvee, pipeline of 10};
\addplot[
color=red,
mark=triangle,
]
table{./results/keyvee/MSET__10_keys___pipeline_50_.results};
\addlegendentry{Keyvee, pipeline of 50};
\end{axis}
\end{tikzpicture}
\caption{MSET Throughput}
\end{figure}
\begin{figure}[!htb]
\begin{tikzpicture}
\begin{axis}[
xmode = log,
xlabel = {Maximum completion time},
ylabel = {Percent of all requests},
legend columns = 4,
transpose legend,
legend style={at={(0.5,-0.2)}, anchor=north}]
	\addplot[
	color=blue,
	mark=diamond,
	]
	table{./results/redis/MSET_10_keys__pipeline_1_w_10.cdf.results};
	\addlegendentry{Redis, 10 clients};
	\addplot[
	color=blue,
	mark=square,
	]
	table{./results/redis/MSET_10_keys__pipeline_1_w_100.cdf.results};
	\addlegendentry{Redis, 100 clients};
	\addplot[
	color=blue,
	mark=triangle,
	]
	table{./results/redis/MSET_10_keys__pipeline_1_w_500.cdf.results};
	\addlegendentry{Redis, 500 clients};
	\addplot[
	color=blue,
	mark=star,
	]
	table{./results/redis/MSET_10_keys__pipeline_1_w_1000.cdf.results};
	\addlegendentry{Redis, 1000 clients};
	\addplot[
	color=red,
	mark=diamond,
	]
	table{./results/keyvee/MSET_10_keys__pipeline_1_w_10.cdf.results};
	\addlegendentry{Keyvee, 10 clients};
	\addplot[
	color=red,
	mark=square,
	]
	table{./results/keyvee/MSET_10_keys__pipeline_1_w_100.cdf.results};
	\addlegendentry{Keyvee, 100 clients};
	\addplot[
	color=red,
	mark=triangle,
	]
	table{./results/keyvee/MSET_10_keys__pipeline_1_w_500.cdf.results};
	\addlegendentry{Keyvee, 500 clients};
	\addplot[
	color=red,
	mark=star,
	]
	table{./results/keyvee/MSET_10_keys__pipeline_1_w_1000.cdf.results};
	\addlegendentry{Keyvee, 1000 clients};
	\end{axis}
	\end{tikzpicture}
	\caption{MSET Latency w/o pipelining}
\end{figure}
Placeholder Text

\subsection{GET Command}

\begin{figure}[!htb]
\begin{tikzpicture}
\begin{axis}[
xlabel = {Parallel Clients},
ylabel = {Throughput (10\textsuperscript{5} Requests/sec)},
legend columns = 3,
transpose legend,
legend style={at={(0.5,-0.2)},anchor=north}]
\addplot[
color=blue,
mark=diamond,
]
table{./results/redis/GET__pipeline_1_.results};
\addlegendentry{Redis, no pipeline};
\addplot[
color=blue,
mark=square,
]
table{./results/redis/GET__pipeline_10_.results};
\addlegendentry{Redis, pipeline of 10};
\addplot[
color=blue,
mark=triangle,
]
table{./results/redis/GET__pipeline_50_.results};
\addlegendentry{Redis, pipeline of 50};
\addplot[
color=red,
mark=diamond,
]
table{./results/keyvee/GET__pipeline_1_.results};
\addlegendentry{Keyvee, no pipeline};
\addplot[
color=red,
mark=square,
]
table{./results/keyvee/GET__pipeline_10_.results};
\addlegendentry{Keyvee, pipeline of 10};
\addplot[
color=red,
mark=triangle,
]
table{./results/keyvee/GET__pipeline_50_.results};
\addlegendentry{Keyvee, pipeline of 50};
\end{axis}
\end{tikzpicture}
\caption{GET Throughput}
\end{figure}
\begin{figure}[!htb]
\begin{tikzpicture}
\begin{axis}[
xmode = log,
xlabel = {Maximum completion time},
ylabel = {Percent of all requests},
legend columns = 4,
transpose legend,
legend style={at={(0.5,-0.2)}, anchor=north}]
	\addplot[
	color=blue,
	mark=diamond,
	]
	table{./results/redis/GET_pipeline_1_w_10.cdf.results};
	\addlegendentry{Redis, 10 clients};
	\addplot[
	color=blue,
	mark=square,
	]
	table{./results/redis/GET_pipeline_1_w_100.cdf.results};
	\addlegendentry{Redis, 100 clients};
	\addplot[
	color=blue,
	mark=triangle,
	]
	table{./results/redis/GET_pipeline_1_w_500.cdf.results};
	\addlegendentry{Redis, 500 clients};
	\addplot[
	color=blue,
	mark=star,
	]
	table{./results/redis/GET_pipeline_1_w_1000.cdf.results};
	\addlegendentry{Redis, 1000 clients};
	\addplot[
	color=red,
	mark=diamond,
	]
	table{./results/keyvee/GET_pipeline_1_w_10.cdf.results};
	\addlegendentry{Keyvee, 10 clients};
	\addplot[
	color=red,
	mark=square,
	]
	table{./results/keyvee/GET_pipeline_1_w_100.cdf.results};
	\addlegendentry{Keyvee, 100 clients};
	\addplot[
	color=red,
	mark=triangle,
	]
	table{./results/keyvee/GET_pipeline_1_w_500.cdf.results};
	\addlegendentry{Keyvee, 500 clients};
	\addplot[
	color=red,
	mark=star,
	]
	table{./results/keyvee/GET_pipeline_1_w_1000.cdf.results};
	\addlegendentry{Keyvee, 1000 clients};
	\end{axis}
	\end{tikzpicture}
	\caption{GET Latency w/o pipelining}
\end{figure}
Talk About GET
\subsection{PING Command}

\begin{figure}[!htb]
\begin{tikzpicture}
\begin{axis}[
xlabel = {Parallel Clients},
ylabel = {Throughput (10\textsuperscript{5} Requests/sec)},
legend columns = 3,
transpose legend,
legend style={at={(0.5,-0.2)},anchor=north}]
\addplot[
color=blue,
mark=diamond,
]
table{./results/redis/PING_BULK__pipeline_1_.results};
\addlegendentry{Redis, no pipeline};
\addplot[
color=blue,
mark=square,
]
table{./results/redis/PING_BULK__pipeline_10_.results};
\addlegendentry{Redis, pipeline of 10};
\addplot[
color=blue,
mark=triangle,
]
table{./results/redis/PING_BULK__pipeline_50_.results};
\addlegendentry{Redis, pipeline of 50};
\addplot[
color=red,
mark=diamond,
]
table{./results/keyvee/PING_BULK__pipeline_1_.results};
\addlegendentry{Keyvee, no pipeline};
\addplot[
color=red,
mark=square,
]
table{./results/keyvee/PING_BULK__pipeline_10_.results};
\addlegendentry{Keyvee, pipeline of 10};
\addplot[
color=red,
mark=triangle,
]
table{./results/keyvee/PING_BULK__pipeline_50_.results};
\addlegendentry{Keyvee, pipeline of 50};
\end{axis}
\end{tikzpicture}
\caption{PING Throughput}
\end{figure}
\begin{figure}[!htb]
\begin{tikzpicture}
\begin{axis}[
xmode = log,
xlabel = {Maximum completion time},
ylabel = {Percent of all requests},
legend columns = 4,
transpose legend,
legend style={at={(0.5,-0.2)}, anchor=north}]
	\addplot[
	color=blue,
	mark=diamond,
	]
	table{./results/redis/PING_BULK_pipeline_1_w_10.cdf.results};
	\addlegendentry{Redis, 10 clients};
	\addplot[
	color=blue,
	mark=square,
	]
	table{./results/redis/PING_BULK_pipeline_1_w_100.cdf.results};
	\addlegendentry{Redis, 100 clients};
	\addplot[
	color=blue,
	mark=triangle,
	]
	table{./results/redis/PING_BULK_pipeline_1_w_500.cdf.results};
	\addlegendentry{Redis, 500 clients};
	\addplot[
	color=blue,
	mark=star,
	]
	table{./results/redis/PING_BULK_pipeline_1_w_1000.cdf.results};
	\addlegendentry{Redis, 1000 clients};
	\addplot[
	color=red,
	mark=diamond,
	]
	table{./results/keyvee/PING_BULK_pipeline_1_w_10.cdf.results};
	\addlegendentry{Keyvee, 10 clients};
	\addplot[
	color=red,
	mark=square,
	]
	table{./results/keyvee/PING_BULK_pipeline_1_w_100.cdf.results};
	\addlegendentry{Keyvee, 100 clients};
	\addplot[
	color=red,
	mark=triangle,
	]
	table{./results/keyvee/PING_BULK_pipeline_1_w_500.cdf.results};
	\addlegendentry{Keyvee, 500 clients};
	\addplot[
	color=red,
	mark=star,
	]
	table{./results/keyvee/PING_BULK_pipeline_1_w_1000.cdf.results};
	\addlegendentry{Keyvee, 1000 clients};
	\end{axis}
	\end{tikzpicture}
	\caption{PING Latency w/o pipelining}
\end{figure}
Talk about Ping

\section{Conclusion}
In this paper we described the implementation and evaluation of our unikernel key-value store, Keyvee.
Our efforts are concentrated around implementing a subset of Redis's functionality using OCaml and the MirageOS toolchain.
We then evaluated our implementation and compared it with Redis using Redis's benchmarking tool.
While similar, if not superior, throughput was achieved in tests having few parallel clients, a scaling issue was found when Keyvee was subjected to larger parallel loads, resulting in poor throughput.
The main source of the low throughput was identified as an underlying issue with Mirage OS's difficulty in handling large numbers of multiple clients.

\section{Acknowledgements}
We thank the kind souls in the \#Mirage IRC chatroom for their relentless patience.
Their assistance was crucial in learning how to use Mirage's utilities and leverage the provided standard library.



\bibliographystyle{abbrv}
\bibliography{bibliography}


\end{document}



